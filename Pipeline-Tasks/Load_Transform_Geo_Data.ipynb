{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd7c08d-64fd-4b8e-be8c-51fbf1fefa4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Databricks_Catalog_Read\").getOrCreate()\n",
    "\n",
    "# Define the catalog and schema\n",
    "catalog_name = \"workspace\"\n",
    "schema_name = \"default\"\n",
    "\n",
    "# Read data from Databricks tables and convert them to spark Dataframe\n",
    "table_full_path = f\"{catalog_name}.{schema_name}.{'geo'}\"\n",
    "    \n",
    "# Read the table using Spark\n",
    "df_geo = spark.read.table(table_full_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49568f3-57df-4a33-8b27-5ae55fc760c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ddc2331-a0ed-4ca6-96a5-ddd06fd73bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, struct, to_timestamp\n",
    "\n",
    "# Check if 'latitude' and 'longitude' exist before creating 'coordinates'\n",
    "if \"latitude\" in df_geo.columns and \"longitude\" in df_geo.columns:\n",
    "    df_geo = df_geo.withColumn(\"coordinates\", struct(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "    #Cast coordinates as string\n",
    "    df_geo = df_geo.withColumn(\"coordinates\", col(\"coordinates\").cast(\"string\"))\n",
    "\n",
    "    # Drop 'latitude' and 'longitude' columns\n",
    "    df_geo = df_geo.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# Convert 'timestamp' column to datetime format\n",
    "df_geo = df_geo.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# Reorder the columns\n",
    "df_geo = df_geo.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "\n",
    "# Show the result\n",
    "df_geo.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5b0b44-018c-4d65-ba9f-8224f8b7cd38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Drop the existing table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS workspace.default.df_geo\")\n",
    "\n",
    "# Save Spark DataFrame as managed delta table\n",
    "df_geo.write.mode(\"overwrite\").saveAsTable(\"workspace.default.df_geo\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Load_Transform_Geo_Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
