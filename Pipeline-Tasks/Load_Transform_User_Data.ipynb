{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29a9878-3007-4b40-966c-8ba01897d4fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Databricks_Catalog_Read\").getOrCreate()\n",
    "\n",
    "# Define the catalog and schema\n",
    "catalog_name = \"workspace\"\n",
    "schema_name = \"default\"\n",
    "\n",
    "# Read data from Databricks tables and convert them to Pandas DataFrames\n",
    "\n",
    "table_full_path = f\"{catalog_name}.{schema_name}.{'user_data'}\"\n",
    "    \n",
    "# Read the table using Spark\n",
    "df_user = spark.read.table(table_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c03af1ce-fd3d-462a-b87d-046dc754f349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9ef221b-49c7-4f44-962e-1cc672a11f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, struct, to_timestamp, concat_ws\n",
    "\n",
    "# Print columns to debug issue\n",
    "print(\"Available columns:\", df_user.columns)\n",
    "\n",
    "# Check if 'first_name' and 'last_name' exist before creating 'user_name'\n",
    "if \"first_name\" in df_user.columns and \"last_name\" in df_user.columns:\n",
    "    df_user = df_user.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "    \n",
    "    # Drop 'first_name' and 'last_name' only if they exist\n",
    "    df_user=df_user.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# Convert 'date_joined' column to datetime format\n",
    "df_user=df_user.withColumn(\"timestamp\",to_timestamp(col(\"date_joined\")))\n",
    "\n",
    "# Reorder the columns\n",
    "df_user = df_user.select('ind', 'user_name', 'age', 'date_joined')\n",
    "\n",
    "# Show the result\n",
    "df_user.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81354f53-977e-4726-ab38-f7347567187e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Drop the existing table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS workspace.default.df_user\")\n",
    "\n",
    "# Save Spark DataFrame as managed delta table\n",
    "df_user.write.mode(\"overwrite\").saveAsTable(\"workspace.default.df_user\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Load_Transform_User_Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
