{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5dfc05-c5ee-4d33-a0ad-e33f2ad17519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when,  regexp_replace, udf, expr, struct, to_timestamp, concat_ws\n",
    "\n",
    "# Initialize Spark session (use Databricks-compatible settings)\n",
    "spark = SparkSession.builder.appName(\"UseSpark\").getOrCreate()\n",
    "\n",
    "# S3 bucket details\n",
    "bucket_name = \"user-b194464884bf-bucket\"\n",
    "\n",
    "# Define the partitions and corresponding DataFrame names\n",
    "partitions = {\n",
    "    \"b194464884bf.geo\": \"df_geo\",\n",
    "    \"b194464884bf.pin\": \"df_pin\",\n",
    "    \"b194464884bf.user\": \"df_user\"\n",
    "}\n",
    "\n",
    "# Read each partition directly into PySpark DataFrames\n",
    "for partition, df_name in partitions.items():\n",
    "    folder_path = f\"s3a://{bucket_name}/topics/{partition}/partition=2/\"\n",
    "    \n",
    "    # Read JSON files from S3 into a PySpark DataFrame\n",
    "    df = spark.read.json(folder_path)\n",
    "\n",
    "    # Create variables dynamically using exec() for display() compatibility\n",
    "    exec(f\"{df_name} = df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d20d7ee-720a-47e1-91c3-40418d3e0792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 1\n",
    "# Define patterns to replace with None\n",
    "replace_patterns = [\n",
    "    (\".*User Info Error.*\", None),\n",
    "    (\".*N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e.*\", None),\n",
    "    (\".*No description available Story format.*\", None),\n",
    "    (\".*No Title Data Available.*\", None),\n",
    "    (\".*Image src error.*\", None),\n",
    "    (\".*No description available.*\",None)\n",
    "]\n",
    "\n",
    "# Iterate over all string columns and apply batch replace\n",
    "for column in df_pin.columns:\n",
    "    # Only process string columns\n",
    "    if dict(df_pin.dtypes)[column] in [\"string\"]:\n",
    "        df_pin = df_pin.withColumn(column, when(col(column) == \"\", None).otherwise(col(column)))  # Replace empty strings with None\n",
    "\n",
    "        for pattern, replacement in replace_patterns:\n",
    "            df_pin = df_pin.withColumn(column, when(col(column).rlike(pattern), None).otherwise(col(column)))\n",
    "\n",
    "\n",
    "#Dropping rows with any null values\n",
    "df_pin = df_pin.dropna(how=\"any\")\n",
    "\n",
    "# Remove \"Local save in \" from the 'save_location' column\n",
    "df_pin = df_pin.withColumn(\"save_location\", regexp_replace(col(\"save_location\"), \"Local save in \", \"\"))\n",
    "\n",
    "#Add \n",
    "df_pin = df_pin.withColumn(\n",
    "    \"follower_count\",\n",
    "    when(col(\"follower_count\").endswith(\"k\"), regexp_replace(col(\"follower_count\"), \"k\", \"000\"))\n",
    "    .when(col(\"follower_count\").endswith(\"M\"), regexp_replace(col(\"follower_count\"), \"M\", \"000000\"))\n",
    "    .otherwise(col(\"follower_count\"))\n",
    ")\n",
    "\n",
    "# Convert the column to integer type\n",
    "df_pin = df_pin.withColumn(\"follower_count\", col(\"follower_count\").cast(\"int\"))\n",
    "\n",
    "# Rename 'index' column to 'ind'\n",
    "df_pin = df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# Reorder columns in the specified order\n",
    "df_pin = df_pin.select(\"ind\", \n",
    "                 \"unique_id\", \n",
    "                 \"title\", \n",
    "                 \"description\", \n",
    "                 \"follower_count\", \n",
    "                 \"poster_name\", \n",
    "                 \"tag_list\", \n",
    "                 \"is_image_or_video\", \n",
    "                 \"image_src\", \n",
    "                 \"save_location\", \n",
    "                 \"category\")\n",
    "\n",
    "\n",
    "\n",
    "#Cast df_pin as string\n",
    "df_pin = df_pin.withColumn(\"ind\", col(\"ind\").cast(\"bigint\"))\n",
    "\n",
    "# Show the result\n",
    "display(df_pin)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2742466-d5bc-4c81-b85d-d2ff9ffc1c20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 2\n",
    "# Check if 'latitude' and 'longitude' exist before creating 'coordinates'\n",
    "if \"latitude\" in df_geo.columns and \"longitude\" in df_geo.columns:\n",
    "    df_geo = df_geo.withColumn(\"coordinates\", struct(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "    #Cast coordinates as string\n",
    "    df_geo = df_geo.withColumn(\"coordinates\", col(\"coordinates\").cast(\"string\"))\n",
    "\n",
    "    # Drop 'latitude' and 'longitude' columns\n",
    "    df_geo = df_geo.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# Convert 'timestamp' column to datetime format\n",
    "df_geo = df_geo.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# Reorder the columns\n",
    "df_geo = df_geo.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "\n",
    "# Show the result\n",
    "df_geo.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e537785c-bf91-438b-8029-124e58413fe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 3\n",
    "# Print columns to debug issue\n",
    "print(\"Available columns:\", df_user.columns)\n",
    "\n",
    "# Check if 'first_name' and 'last_name' exist before creating 'user_name'\n",
    "if \"first_name\" in df_user.columns and \"last_name\" in df_user.columns:\n",
    "    df_user = df_user.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "    \n",
    "    # Drop 'first_name' and 'last_name' only if they exist\n",
    "    df_user=df_user.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# Convert 'date_joined' column to datetime format\n",
    "df_user=df_user.withColumn(\"timestamp\",to_timestamp(col(\"date_joined\")))\n",
    "\n",
    "# Reorder the columns\n",
    "df_user = df_user.select('ind', 'user_name', 'age', 'date_joined')\n",
    "\n",
    "# Show the result\n",
    "df_user.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "957f52ce-b0d0-4922-83ff-c743da30a4f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure all DataFrames have at least 1000 rows before concatenation\n",
    "# Initialize Spark session (if not already initialized)\n",
    "spark = SparkSession.builder.appName(\"CombineDataFrames\").getOrCreate()\n",
    "\n",
    "# Limit each DataFrame to 1000 rows (equivalent to `.head(1000)`)\n",
    "df_user_limited = df_user.limit(1000)\n",
    "df_geo_limited = df_geo.limit(1000)\n",
    "df_pin_limited = df_pin.limit(1000)\n",
    "\n",
    "# Add a row number column to each DataFrame to ensure proper alignment\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.orderBy(\"some_unique_column\")  # Replace with a meaningful column for ordering\n",
    "\n",
    "df_user_limited = df_user_limited.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "df_geo_limited = df_geo_limited.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "df_pin_limited = df_pin_limited.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# Perform a column-wise join using the row number as the key\n",
    "df_combined = df_pin_limited.join(df_user_limited, \"row_num\", \"inner\").join(df_geo_limited, \"row_num\", \"inner\")\n",
    "\n",
    "# Drop the helper row number column\n",
    "df_combined = df_combined.drop(\"row_num\")\n",
    "\n",
    "display(df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f51a11-446e-4309-b030-a1b555a957f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 4\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PopularCategory\").getOrCreate()\n",
    "\n",
    "# Assuming df_combined is your existing PySpark DataFrame\n",
    "# Group by country and category, then count occurrences\n",
    "df_category_count = df_combined.groupBy(\"country\", \"category\").agg(count(\"*\").alias(\"category_count\"))\n",
    "\n",
    "# Define a window specification to get the most popular category in each country\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(col(\"category_count\").desc())\n",
    "\n",
    "# Add row number based on category count within each country\n",
    "df_most_popular_category_by_country = df_category_count.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filter the top-ranked category for each country\n",
    "df_most_popular_category_by_country = df_most_popular_category_by_country.filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Show the result\n",
    "display(df_most_popular_category_by_country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b328b0c-9d80-4399-abf2-c6d2745bb2be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 5\n",
    "# Convert timestamp column to datetime format\n",
    "df_combined = df_combined.withColumn(\"post_year\", year(col(\"timestamp\")))\n",
    "\n",
    "# Group by year and category, then count occurrences\n",
    "df_category_count = df_combined.groupBy(\"post_year\", \"category\").agg(count(\"*\").alias(\"category_count\"))\n",
    "\n",
    "# Define a window specification to find the most popular category per year\n",
    "window_spec = Window.partitionBy(\"post_year\").orderBy(col(\"category_count\").desc())\n",
    "\n",
    "# Assign a ranking to each category within each year\n",
    "df_ranked = df_category_count.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filter only the most popular category (rank = 1) per year\n",
    "df_most_popular_category_by_year = df_ranked.filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "display(df_most_popular_category_by_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43bc9a21-7074-4aeb-a84f-012393d68600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 6\n",
    "#Step 1\n",
    "#Find the user with most followers in each country\n",
    "\n",
    "# Group by country and poster_name, then sum up follower counts\n",
    "df_followers = df_combined.groupBy(\"country\", \"poster_name\").agg(sum(\"follower_count\").alias(\"total_followers\"))\n",
    "\n",
    "# Define a window specification to rank users by follower count within each country\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(col(\"total_followers\").desc())\n",
    "\n",
    "# Assign ranking to users within each country\n",
    "df_ranked = df_followers.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filter only the top-ranked user per country\n",
    "df_top_users_per_country = df_ranked.filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "display(df_top_users_per_country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2695845e-5915-442f-ad3b-78b979810f09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 6\n",
    "#Step 2\n",
    "# Define a window specification to find the country with the highest follower count\n",
    "window_spec = Window.orderBy(col(\"total_followers\").desc())\n",
    "\n",
    "# Assign a ranking based on the highest follower count across all countries\n",
    "df_ranked = df_top_users_per_country.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filter only the country with the highest follower count\n",
    "df_top_country = df_ranked.filter(col(\"rank\") == 1).select(\"country\", \"total_followers\").drop(\"rank\")\n",
    "\n",
    "display(df_top_country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abcb9638-4855-4f3b-adc0-b549d189ada3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 7\n",
    "\n",
    "# Define age bins using PySpark\n",
    "df_combined = df_combined.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") <= 24, \"18-24\")\n",
    "    .when((col(\"age\") > 24) & (col(\"age\") <= 35), \"25-35\")\n",
    "    .when((col(\"age\") > 35) & (col(\"age\") <= 50), \"36-50\")\n",
    "    .otherwise(\"50+\")\n",
    ")\n",
    "\n",
    "# Group by age_group and category, then count occurrences\n",
    "df_category_count = df_combined.groupBy(\"age_group\", \"category\").agg(count(\"*\").alias(\"category_count\"))\n",
    "\n",
    "# Define a window specification to rank categories within each age group\n",
    "window_spec = Window.partitionBy(\"age_group\").orderBy(col(\"category_count\").desc())\n",
    "\n",
    "# Assign ranking to categories within each age group\n",
    "df_ranked = df_category_count.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filter only the most popular category per age group\n",
    "df_most_popular_category_by_age_group = df_ranked.filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Show the result\n",
    "df_most_popular_category_by_age_group.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3759f9d-1e79-4358-abe3-fa457e116fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 8\n",
    "# Create a new DataFrame with age groups (without modifying df_combined)\n",
    "df_age_grouped = df_combined.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") <= 24, \"18-24\")\n",
    "    .when((col(\"age\") > 24) & (col(\"age\") <= 35), \"25-35\")\n",
    "    .when((col(\"age\") > 35) & (col(\"age\") <= 50), \"36-50\")\n",
    "    .otherwise(\"50+\")\n",
    ")\n",
    "\n",
    "# Compute the median follower count for each age group using approxQuantile\n",
    "df_median_followers = df_age_grouped.groupBy(\"age_group\").agg(\n",
    "    expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\")\n",
    ")\n",
    "\n",
    "# Define a custom sorting column based on the age_group order\n",
    "df_median_followers = df_median_followers.withColumn(\n",
    "    \"sort_order\",\n",
    "    when(col(\"age_group\") == \"18-24\", 1)\n",
    "    .when(col(\"age_group\") == \"25-35\", 2)\n",
    "    .when(col(\"age_group\") == \"36-50\", 3)\n",
    "    .when(col(\"age_group\") == \"50+\", 4)\n",
    ")\n",
    "\n",
    "# Sort by the custom order and drop the helper column\n",
    "df_median_followers = df_median_followers.orderBy(\"sort_order\").drop(\"sort_order\")\n",
    "\n",
    "# Show the sorted result\n",
    "df_median_followers.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11acec22-9851-4e9d-95e6-d909c4c70329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 9\n",
    "# Convert 'date_joined' column to year (extracting the year)\n",
    "df_with_year = df_combined.withColumn(\"post_year\", year(col(\"date_joined\")))\n",
    "\n",
    "# Filter data for years between 2015 and 2020\n",
    "df_filtered = df_with_year.filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020))\n",
    "\n",
    "# Count number of users joined per year\n",
    "df_users_per_year = df_filtered.groupBy(\"post_year\").agg(count(\"*\").alias(\"number_users_joined\"))\n",
    "\n",
    "# Sort by year (optional, to ensure ascending order)\n",
    "df_users_per_year = df_users_per_year.orderBy(\"post_year\")\n",
    "\n",
    "# Show the result\n",
    "df_users_per_year.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4527686c-53d7-4a28-9fd5-ccda162a7932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 10\n",
    "# Convert 'date_joined' column to extract year\n",
    "df_with_year = df_combined.withColumn(\"post_year\", year(col(\"date_joined\")))\n",
    "\n",
    "# Filter data for users who joined between 2015 and 2020\n",
    "df_filtered = df_with_year.filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020))\n",
    "\n",
    "# Calculate median follower count per year using approxQuantile\n",
    "df_median_followers = df_filtered.groupBy(\"post_year\").agg(\n",
    "    expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\")\n",
    ")\n",
    "\n",
    "# Sort by year (optional, to ensure ascending order)\n",
    "df_median_followers = df_median_followers.orderBy(\"post_year\")\n",
    "\n",
    "# Show the result\n",
    "df_median_followers.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba27a9c1-0750-4974-a615-1fb2d56c6bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 11\n",
    "# Convert date_joined column to datetime format and extract the year\n",
    "df_combined['post_year'] = pd.to_datetime(df_combined['date_joined']).dt.year\n",
    "\n",
    "# Filter data for users who joined between 2015 and 2020\n",
    "df_filtered = df_combined[(df_combined['post_year'] >= 2015) & (df_combined['post_year'] <= 2020)]\n",
    "\n",
    "# Group by post_year and age_group, then calculate the median follower count\n",
    "df_median_followers_by_age_group = df_filtered.groupby(['post_year', 'age_group'])['follower_count'].median().reset_index(name='median_follower_count')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_median_followers_by_age_group\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pinterest Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
